{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Autolab is an open-source automated code grading platform developed at Carnegie Mellon University and used by universities around the world. The UB CSE IT department maintains a customized variant of Autolab for use in our courses. It's hosted on premises at UB, and you can access it at https://autolab.cse.buffalo.edu.</p> <p>This documentation will serve as a guide, primarily for instructors, on how to use Autolab. It covers nearly everything an instructor may be interested in, from the basics to more advanced features. If you have any questions after consulting the documentation, feel free to contact us.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Your First Login</li> <li>Create a Course</li> </ul>"},{"location":"#course-management","title":"Course Management","text":"<ul> <li>Viewing Your Courses</li> <li>Course Page</li> <li>Manage Course</li> <li>Course Settings</li> <li>Enrolling Students<ul> <li>Using the CSE IT Classlists Tool</li> <li>Uploading a CSV File</li> </ul> </li> <li>Enrolling Teaching Assistants</li> <li>Additional Course Management Features</li> </ul>"},{"location":"#create-an-assessment","title":"Create an Assessment","text":"<ul> <li>Install Assessment<ul> <li>Import from Tarball</li> <li>Create from Scratch</li> </ul> </li> <li>Assessment Page</li> <li>Edit Assessment<ul> <li>Basic</li> <li>Handin</li> <li>Penalties</li> <li>Problems</li> <li>Advanced</li> </ul> </li> <li>Add Problems</li> <li>Make a Submission</li> <li>View, Annotate, and Grade a Submission<ul> <li>View a Submission</li> <li>Annotate a Submission</li> <li>Assign a Grade by Annotation</li> <li>Stacking Annotations</li> </ul> </li> </ul>"},{"location":"#create-an-autograder","title":"Create an Autograder","text":"<ul> <li>Setup<ul> <li>Create an Assessment</li> <li>Set the Handin Filename</li> <li>Add some Problems</li> <li>Add an Autograder</li> </ul> </li> <li>A Minimal Autograder<ul> <li>grader.py</li> <li>autograde.tar</li> <li>Makefile</li> </ul> </li> <li>Upload the Autograder to Autolab</li> <li>Test the Autograder<ul> <li>Upload a Correct Solution</li> <li>Upload an Incorrect Solution</li> <li>Upload a Problematic Solution<ul> <li>Crashing the Grader</li> <li>Getting the Grader's Source Code</li> <li>Setting an Arbitrary Score</li> </ul> </li> </ul> </li> <li>Formatted Feedback</li> <li>Embedded Forms<ul> <li>Create the HTML Form</li> <li>Upload the HTML Form</li> <li>Considerations for the Autograder</li> <li>Embedded form with file submission (UB Feature)</li> </ul> </li> </ul>"},{"location":"#grader-assignment-tool","title":"Grader Assignment Tool","text":"<ul> <li>Accessing the GAT</li> <li>Import Your Course from Autolab</li> <li>Manage People</li> <li>Manage Conflicts of Interest</li> <li>Create a Grading Assignment</li> <li>View a Grading Assignment</li> <li>Archive a Grading Assignment</li> </ul>"},{"location":"#ub-course-sections","title":"UB Course Sections","text":"<ul> <li>Configure your Course Sections<ul> <li>Magic Import</li> <li>Create a Section Manually</li> </ul> </li> <li>Configure an Assessment</li> <li>How it Looks to Students</li> </ul>"},{"location":"#create-an-autograding-image","title":"Create an Autograding Image","text":"<ul> <li>Creating your Own Image</li> <li>Testing your Image</li> <li>Add your Image to Autolab</li> </ul>"},{"location":"#the-autograding-process","title":"The Autograding Process","text":"<ul> <li>Sending the Job to Tango</li> <li>Tango Queues the Job</li> <li>Running the Job</li> <li>Autodriver</li> <li>After the Job is Finished</li> </ul>"},{"location":"#sample-autograders","title":"Sample Autograders","text":"<p>Sample autograders are located in the <code>sample_files</code> directory of the GitHub repository for this documentation. These are referenced throughout the documentation. Some autograder directories contain multiple correct and/or incorrect solutions to demonstrate how the grader works in different situations.</p> <ul> <li>autograder0 is the most minimal autograder just to give you an idea of the format.</li> <li>autograder1 is a more realistic autograder that grades 3 problems differently, but it's   not robust enough for real use.</li> <li>autograder2 demonstrates how to use the settings.json file to get metadata about the   submission.</li> <li>autograder3 demonstrates how to grade an embedded form submission.</li> <li>autograder4 demonstrates how to create an embedded form that allows submitting a file.</li> </ul>"},{"location":"#private-documentation","title":"Private documentation","text":"<p>For IT staff, there's also a private internal documentation repository available here: https://github.com/UB-CSE-IT/Autolab-Internal-Docs.</p>"},{"location":"Course%20management/","title":"Course Management","text":""},{"location":"Course%20management/#viewing-your-courses","title":"Viewing Your Courses","text":"<p>The Autolab homepage will redirect you to https://autolab.cse.buffalo.edu/courses if you're enrolled in at least one course. From here, you can see current, completed, and upcoming courses that you're involved in. You won't see courses where you're not enrolled as an instructor, teaching assistant (Autolab refers to this as CA: \"Course Assistant\") or student. You can click on the blue banner or the \"Course Page\" button to go to the course page. There's also a shortcut button to jump immediately to the gradebook.</p> <p></p> <p>If you're an instructor or CA, you'll see a badge on the top right of the course. Above, you can see that I'm an instructor since I just created this course by following the steps in Getting Started &gt; Create a Course. Below is what a CA will see. We'll cover enrolling students, CAs, and additional instructors shortly.</p> <p></p> <p>Students will not see a badge:</p> <p></p>"},{"location":"Course%20management/#course-page","title":"Course Page","text":"<p>Navigate to your course page, and you'll see the empty course dashboard.</p> <p></p>"},{"location":"Course%20management/#manage-course","title":"Manage Course","text":"<p>Let's start configuring the course. Click the <code>Manage Course</code> button on the navigation bar. Here's what you'll see:</p> <p></p> <p>This will be referred to as the \"Manage Course\" page throughout the documentation.</p>"},{"location":"Course%20management/#course-settings","title":"Course Settings","text":"<p>From the manage course page, click <code>Course settings</code> to configure the general settings for the course. You can look through all the options, but the default values are reasonable for most courses. Each option has hint text below it to explain what it does.</p> <p>The one thing I'd recommend doing now is adding your course homepage URL to the \"Website\" field. This will enable the \"Course Website\" button on the course dashboard.</p> <p></p> <p>When you're done tweaking the settings, click <code>Save</code> at the bottom of the page. I won't keep mentioning this throughout the documentation, but note that you'll need to click <code>Save</code> on most pages to save your changes.</p>"},{"location":"Course%20management/#enrolling-students","title":"Enrolling Students","text":"<p>From the manage course page, click <code>Manage course uesrs</code>. You'll see a list of all the users enrolled in the course. This will initially just be you and some CSE IT staff members.</p> <p></p> <p>Click <code>Import Roster/Users</code> to import a roster of students. You'll see a page with instructions for formatting a CSV to import, but CSE IT already has a tool that will build this CSV file for you. I'll walk through using that.</p> <p></p>"},{"location":"Course%20management/#using-the-cse-it-classlists-tool","title":"Using the CSE IT Classlists Tool","text":"<p>(Using this tool is not required. You can manually create a roster CSV file by following the format on the import page.)</p> <p>Click the CSE IT's Classlists tool link on the page. Note that only CSE faculty and staff can access this tool.</p> <ol> <li>Select the term</li> <li>Choose to either only show your classes or all classes. You may need to show all classes if you and another    instructor are co-teaching a course.</li> <li>Click next</li> </ol> <p></p> <p>Select the course you want the roster for, then scroll down and click <code>Next</code>.</p> <p></p> <ol> <li>Select the \"AutoGrader Inputs\" tab</li> <li>Click the blue button, which includes all sections of the course, to download the CSV file.</li> </ol> <p></p> <p>Go back to the Classlists tool and repeat the steps to download a CSV for each main section of the course.</p> <p></p>"},{"location":"Course%20management/#uploading-a-csv-file","title":"Uploading a CSV file","text":"<p>Unfortunately, we can only upload one CSV file at a time, so this process will need to be repeated for each CSV file.</p> <p>Once you've downloaded or otherwise obtained a CSV file, go back to the Autolab import page and click the <code>Browse</code> button, and then the <code>Upload</code> button to upload the CSV files.</p> <p></p> <p>Review the changes that will be made, and then click <code>Confirm</code> if the data looks correct.</p> <p></p> <p>Now you'll see the full roster with the new students added.</p> <p></p>"},{"location":"Course%20management/#enrolling-teaching-assistants","title":"Enrolling Teaching Assistants","text":"<p>Navigate to the \"Manage Course Users\" page.</p> <ol> <li>Click <code>Add Users from Emails</code>.</li> <li>Enter the Buffalo email addresses of the TAs you want to add, one per line.</li> <li>Choose either \"Course Assistant\" or \"Instructor\" for their role.<ul> <li>Instructors have full access to the course, equivalent to yours (assuming you're the instructor who created this   course).</li> <li>CAs can only assign scores to problems.</li> </ul> </li> <li>Click <code>Submit</code>.</li> </ol> <p></p> <p>You'll see the TAs added to the roster.</p> <p></p>"},{"location":"Course%20management/#side-note-on-autolab-accounts","title":"Side Note on Autolab Accounts","text":"<ul> <li>In this example, the accounts don't really exist, so the names are incorrect/missing.</li> <li>If the TA already has an Autolab account, which is highly likely, the data will be correct.</li> <li>If the TA doesn't have an Autolab account, their information will automatically be updated when they sign in for the     first time.</li> <li>The same applies to students if you create their accounts this way (without the CSV), but fields like the lecture     and section won't be filled. It's highly recommended to enroll students with the CSV import feature.</li> </ul>"},{"location":"Course%20management/#additional-course-management-features","title":"Additional Course Management Features","text":"<p>From the manage course page, you can notably also:</p> <ul> <li>Manage course attachments to provide students with files</li> <li>Manage announcements to show students a short message on the course dashboard</li> <li>Act as another user to see what the course looks like from their perspective</li> <li>Run Moss cheat detection to look for similar code submissions between students</li> </ul>"},{"location":"Course%20management/#whats-next","title":"What's Next?","text":"<p>Now that you've created a course and enrolled students, you'll probably want to Create an Assessment.</p>"},{"location":"Create%20an%20assessment/","title":"Create an Assessment","text":"<p>An Autolab assessment is a graded assignment that students can hand in. They can be used for homework, labs, exams, and more. They can be manually graded or automatically graded.</p>"},{"location":"Create%20an%20assessment/#install-assessment","title":"Install Assessment","text":"<p>Navigate to your course dashboard and click <code>Install Assessment</code>.</p> <p></p> <p>You'll see the various ways you can add an assessment to your course.</p> <p></p>"},{"location":"Create%20an%20assessment/#import-from-tarball","title":"Import from tarball","text":"<p>If you have an assessment from a previous semester, you can export it from the previous course as a tarball and then import it here. Just click <code>Browse</code> and select the tarball.</p>"},{"location":"Create%20an%20assessment/#side-note-about-import-from-file-system","title":"Side note about \"import from file system\"","text":"<p>This feature is only accessible to users who have direct access to the Autolab server, which essentially means we don't use it. If there's a particular need for this, contact CSE IT.</p>"},{"location":"Create%20an%20assessment/#create-from-scratch","title":"Create from scratch","text":"<p>This is how assessments should be initially created. Click the \"Assessment Builder\" link to begin.</p> <ol> <li>Choose the assessment name, which is what students will see. Naming it something more specific than \"Homework 1\" is    recommended. Something like \"Homework 1: HTTP Basics\" may be more identifiable to students.</li> <li>Choose the assessment category. If you already have categories defined, you can choose one of those from the    dropdown. Otherwise, you can create a new category by typing in a new name. Categories are used to group assessments.    Popular categories include Homework, Labs, Exams, and Projects. Separating assessments into categories is useful for    weighted grading if you want to use some advanced features of Autolab. Otherwise, it's just good for organization.</li> <li>Click <code>Create Assessment</code>.</li> </ol> <p>You can change any of these settings later, except for the URL, which will be automatically generated based on the display name you choose.</p> <p></p>"},{"location":"Create%20an%20assessment/#assessment-page","title":"Assessment Page","text":"<p>After creating an assessment, you'll be brought to the assessment page. You have a lot of options here.</p> <p></p>"},{"location":"Create%20an%20assessment/#edit-assessment","title":"Edit Assessment","text":"<p>Let's start by looking at everything you can customize about the assessment. Click <code>Edit Assessment</code> at the top of the \" Admin Options.\" Here's an overview.</p>"},{"location":"Create%20an%20assessment/#basic","title":"Basic","text":"<p>The \"basic\" tab allows you to edit the values you chose while creating the assessment and change some other display options. They're fairly self-explanatory with the help text below each option. The ability to upload a custom Ruby file for the assessment is advanced, so you can ignore that for now.</p> <p></p>"},{"location":"Create%20an%20assessment/#handin","title":"Handin","text":"<p>The \"handin\" tab allows you to configure the start, due, end, and grading deadline dates for the assessment. You can choose if students are allowed to submit directly from a GitHub repo, change the handin filename, set the maximum submission size, and disable handins. Each of these is explained in the help text below the option.</p> <p></p>"},{"location":"Create%20an%20assessment/#the-handin-filename-field-is-important","title":"The <code>Handin filename</code> field is important","text":"<ul> <li>It doesn't matter what the file is named on a student's computer; it will be renamed to the handin filename when     they upload it.</li> <li>If you're using an automatic grader, this is the name of the file that will be accessible in the grading VM.</li> <li>Students will be warned before uploading a file that doesn't match the handin filename's extension.</li> </ul>"},{"location":"Create%20an%20assessment/#penalties","title":"Penalties","text":"<p>The \"penalties\" tab allows you to configure the maximum submissions limit and reduce scores on late submissions.</p> <p></p>"},{"location":"Create%20an%20assessment/#problems","title":"Problems","text":"<p>The \"problems\" tab allows you to add problems to the assessment, which are the actual \"questions\" students will answer and receive points for. We'll add some to our new assessment after this overview.</p> <p></p>"},{"location":"Create%20an%20assessment/#advanced","title":"Advanced","text":"<p>The \"advanced\" tab allows you to configure student groups and use an embedded HTML form for submissions.</p> <p>The official Autolab Embedded Forms documentation is a good reference.</p> <p></p>"},{"location":"Create%20an%20assessment/#add-problems","title":"Add Problems","text":"<p>Now that we've looked at the assessment settings, let's add some problems. Navigate to the \"Problems\" tab and click <code>Add Problem</code>.</p> <p></p> <ol> <li>Name the problem. This is what students will see. You'll also use this name in your autograder output.</li> <li>Set the maximum score possible for this problem.</li> <li>Click <code>Save Problem</code>. All settings can be changed later.</li> </ol> <p>You can optionally set a description or make the problem optional.</p> <p></p> <p>I've gone ahead and created another problem, so we can see both in the problems list.</p> <p></p> <p>Now the assessment is ready for students to submit to. We'll need to get some student submissions before we can grade them. You can also make submissions as an instructor, which is useful for testing.</p>"},{"location":"Create%20an%20assessment/#make-a-submission","title":"Make a Submission","text":"<p>(I've updated the assessment start and end date so that we can make a submission in the expected timeframe and not have to deal with late days. I also updated the handin filename to \"handin.pdf\" to indicate that we want a PDF submission from students.)</p> <p>Navigate to the assessment page.</p> <ol> <li>Select (or drag and drop) a file to upload.</li> <li>Affirm that you're complying with the academic integrity policy.</li> <li>Click <code>Submit</code>.</li> </ol> <p></p> <p>You'll be redirected to the list of your submissions.</p> <p></p>"},{"location":"Create%20an%20assessment/#view-annotate-and-grade-a-submission","title":"View, Annotate, and Grade a Submission","text":""},{"location":"Create%20an%20assessment/#view-a-submission","title":"View a submission","text":"<p>To view, annotate, and/or grade a submission, click the <code>View Source</code> button on it.</p> <p></p> <p>You'll be able to see the submission rendered in the browser. You can also download the submission and navigate between submissions and different students with the buttons at the top.</p> <p></p>"},{"location":"Create%20an%20assessment/#annotate-a-submission","title":"Annotate a submission","text":"<p>You can click somewhere on the submission to create an annotation and assign a grade. Students will be able to review this to learn why they were scored the way they were.</p> <p>In a code submission, you can create annotations per line. In a PDF submission, you can create annotations anywhere.</p> <p>You need to assign a score and question to each annotation. The way it affects the student's score will depend on the grading mode you selected in the problems tab of the assessment settings.</p> <p></p> <p>Recall we're using the \"Negative Grading\" mode, which is the default value. This means that assigning a score of 0 (or <code>None</code>) is perfect; it reduces the score by 0. If we wanted to reduce the score, we could assign a negative score.</p> <p>In the \"positive grading\" mode, the student starts with a score of 0 and annotations add to the score.</p> <p></p>"},{"location":"Create%20an%20assessment/#assign-a-grade-by-annotation","title":"Assign a grade by annotation","text":"<p>Now you can create one (or more) annotations per problem to assign scores. You'll see the totals computed on the right sidebar. Unlike most things in Autolab, it's saved immediately after adding the annotation.</p> <p></p>"},{"location":"Create%20an%20assessment/#stacking-annotations","title":"Stacking annotations","text":"<p>We can combine various positive and negative scores to create a final score for the problem. Maybe a student missed part of the question but answered another part exceptionally well.</p> <p></p>"},{"location":"Create%20an%20autograder/","title":"Create an Autograder","text":"<p>This is arguably the most important feature of Autolab. An autograder will run automatically after a student makes a submission to an assessment. As an instructor, you will write code that grades a student's submission. When a student submits their work, it will be copied into a Docker container, along with your grading files and some metadata. Your code should interact with the student's submission in some way to determine its correctness. You can print feedback to stdout for the student to see. The final line you print must be a JSON string with the student's scores for each problem in the assessment.</p>"},{"location":"Create%20an%20autograder/#setup","title":"Setup","text":""},{"location":"Create%20an%20autograder/#create-an-assessment","title":"Create an assessment","text":"<p>Start by creating an assessment as described in the Create an Assessment guide.</p> <p>From the assessment page, click <code>Edit assessment</code>.</p> <p></p>"},{"location":"Create%20an%20autograder/#set-the-handin-filename","title":"Set the handin filename","text":"<p>Choose the handin filename from the \"Handin\" tab. This is the name of the file that your autograder will need to interact with. The extension is an important hint about what type of file students should upload. In this simple grader, I'll have the students upload a Python script.</p> <p></p>"},{"location":"Create%20an%20autograder/#have-a-multi-file-project","title":"Have a multi-file project?","text":"<p>Since students can only submit one file, it's common to request they upload a .zip or .tar.gz archive for larger projects. Then, your grader can decompress the archive and have access to all the files.</p>"},{"location":"Create%20an%20autograder/#add-some-problems","title":"Add some problems","text":"<p>From the <code>Problems</code> tab, add some problems that will be graded by our autograder.</p> <p></p>"},{"location":"Create%20an%20autograder/#save-your-changes","title":"Save your changes","text":"<p>Remember, Autolab doesn't apply most settings immediately. You'll need to click <code>Save</code> at the bottom of the page after making these changes.</p>"},{"location":"Create%20an%20autograder/#add-an-autograder","title":"Add an autograder","text":"<p>From the <code>Basic</code> tab, scroll down to the \"Modules Used\" section, and click the plus next to \"Autograder\".</p> <p></p> <ol> <li>Choose a VM image to use. This is the Docker image that your container will be built from. The <code>autograding_image</code> is    a good choice for Python projects.<ul> <li>You can see all the Dockerfiles for Autolab's VM images in our   Autograding Images repository. If you need a custom image,   see Create an autograding image. Try to use an existing image if possible.</li> </ul> </li> <li>Choose the timeout for the autograder. This is the maximum amount of time your autograder will be allowed to run. If    grading takes longer than this, the student will receive partial feedback and an error saying that Autolab couldn't    parse the output from your grader.<ul> <li>Our new hardware as of fall 2023 is much faster, and many jobs finish in under 10 seconds. I recommend uploading   your own solution to the assessment and running it to get an idea of how long it takes to run. Then, at your   discretion, add some time to account for performance variation and inefficient submissions.</li> <li>If you want to grade based on runtime, this is not the way to do it. You'll need to implement the timeout feature   within your grader. Since there's variation between runs, consider even timing your solution within the same   container and adjust the expected runtime dynamically.</li> </ul> </li> <li>Click <code>Save Settings</code>. We'll come back to this page later to upload the grader.</li> </ol> <p></p>"},{"location":"Create%20an%20autograder/#a-minimal-autograder","title":"A Minimal Autograder","text":"<p>An Autograder is composed of two files: a Makefile, and a .tar (not gzipped) archive containing your arbitrary grading code.</p> <p>Let's walk through the process of creating a super simple autograder. In the real world, you'll need to make your graders more robust.</p> <p>For this assignment, students will upload a Python script with a function named <code>get_positive_number</code> that should return any positive number of their choosing.</p> <p>This autograder's files are available in the <code>sample_files/autograder0</code> directory if you want to experiment with it.</p>"},{"location":"Create%20an%20autograder/#graderpy","title":"grader.py","text":"<p>You're free to design your grading code however you want. There are no hard requirements for structuring your grader as long as it can be initiated from a Makefile. For example, there's nothing special about the name \"grader.py\". There are, however, specific ways to format the final output, which your grader must output to stdout.</p> <p>The final line your grader outputs to stdout must be a JSON string formatted like this, where q1, q2, and q3 are the problem names in the assessment:</p> <pre><code>{\"scores\": {\"q1\": 10, \"q2\": 10, \"q3\": 10}}\n</code></pre> <p>There are more advanced feedback features too, but this is the bare minimum.</p> <p>You can review my sample <code>grader.py</code> in the <code>sample_files/autograder0</code> directory.</p>"},{"location":"Create%20an%20autograder/#autogradetar","title":"autograde.tar","text":"<p>Once you've written your grader of arbitrary complexity, you'll need to package it into a .tar (not gzipped) archive. I provided a bash script located at sample_files/autograder0/create_grader.sh that will create the archive in this case. It just runs one simple command to bundle all the grading files into an archive. (In this case, there's only one file.):</p> <p>This file MUST be named <code>autograde.tar</code>. Autolab will rename it when you upload it, so your Makefile and anything else that depends on this must expect the name to be <code>autograde.tar</code>.</p> <pre><code>tar -cf autograde.tar grader.py\n</code></pre>"},{"location":"Create%20an%20autograder/#makefile","title":"Makefile","text":"<p>The Makefile is the entry point for an Autograder. After copying the student's submission and your .tar grader file into a directory, Autolab will run <code>make</code> in that directory. This should invoke the grading process.</p> <p>Here's a minimal Makefile that will extract our grader and run <code>grader.py</code>.</p> <pre><code>all:\n    tar -xf autograde.tar\n    python3 grader.py\n</code></pre>"},{"location":"Create%20an%20autograder/#upload-the-autograder-to-autolab","title":"Upload the Autograder to Autolab","text":"<p>Now that we have our autograde.tar and Makefile, we can upload them to Autolab.</p> <p>Since we added the Autograder module to the assessment, we can click <code>Autograder settings</code> from the assessment page.</p> <p></p> <p>Upload the autograde.tar and Makefile, then click <code>Save Settings</code>.</p> <p>Tip: You can drag the files onto the button instead of needing to open a file browser twice.</p> <p></p> <p>Our assessment is now ready to grade submissions!</p>"},{"location":"Create%20an%20autograder/#test-the-autograder","title":"Test the Autograder","text":""},{"location":"Create%20an%20autograder/#upload-a-correct-solution","title":"Upload a correct solution","text":"<p>Let's upload a correct solution to verify that our autograder works. There's a sample solution available at <code>sample_files/autograder0/handin.py</code>.</p> <p>From the assessment page, choose the file to submit, agree to the academic integrity policy, and click <code>Submit</code>.</p> <p></p> <p>You'll be redirected to your handin history.</p> <p></p> <p>Refresh the page in about 3 seconds, and you'll see your scores.</p> <p></p> <p>Click one of the scores to view your feedback. For autograded assignments, each problem has the same feedback.</p> <p></p>"},{"location":"Create%20an%20autograder/#upload-an-incorrect-solution","title":"Upload an incorrect solution","text":"<p>Now, let's upload an incorrect solution to verify that our autograder doesn't award points.</p> <p>There's a sample incorrect solution available at <code>sample_files/autograder0/handin_incorrect1.py</code>. Follow the same steps as before to upload this.</p> <p></p> <p>Good! Our autograder is working as expected.</p>"},{"location":"Create%20an%20autograder/#upload-a-problematic-solution","title":"Upload a problematic solution","text":""},{"location":"Create%20an%20autograder/#crashing-the-grader","title":"Crashing the grader","text":"<p>Now, let's upload a solution that will cause our autograder to fail. This will serve to emphasize the importance of writing robust graders. I wrote a sample solution that doesn't contain the method our grader expects to call. It's available at <code>sample_files/autograder0/handin_incorrect2.py</code>.</p> <p>Here's the output we'll get from that:</p> <p></p> <p>Autolab tried to parse the last line as a scores JSON string, but it failed because our grader crashed before it printed the scores. In this case, each problem will be assigned a zero.</p> <p>This may not sound bad at first, but the student can see the full traceback, which may leak information about your grader and make it easy for them to hardcode solutions. It's very important to make your graders robust so they won't crash or leak sensitive information. I'll demonstrate some other naive leaking techniques below. There are more advanced attacks that won't be discussed publicly here.</p>"},{"location":"Create%20an%20autograder/#getting-the-graders-source-code","title":"Getting the grader's source code","text":"<p>For example, with this non-robust grader, the <code>sample_files/autograder0/handin_cheating1.py</code> solution will print grader.py's source code for the student to inspect.</p> <p></p>"},{"location":"Create%20an%20autograder/#setting-an-arbitrary-score","title":"Setting an arbitrary score","text":"<p>If a student is able to print the final line of the grader's output, they can set their own score. This is possible if you're showing the student's output, and they're able to freeze the autograder until it times out.</p> <p>For example, with this non-robust grader, the <code>sample_files/autograder0/handin_cheating2.py</code> solution will print <code>{\"scores\": {\"q1\": 10, \"q2\": 9999, \"q3\": 9999}}</code> and then sleep for a long time. The grader will time out and nothing else will be printed, which means Autolab sees that line as the student's scores. You need to ensure your grader outputs the final line, which usually means implementing your own timeouts.</p> <p></p>"},{"location":"Create%20an%20autograder/#formatted-feedback","title":"Formatted Feedback","text":"<p>The new version of Autolab (fall 2023) supports formatted feedback. This is a much more user-friendly way to show autograding results.</p> <p>To enable the most basic form of formatted feedback, you'll need to print another JSON map to stdout just before the scores:</p> <pre><code>{\"_presentation\": \"semantic\"}\n</code></pre> <p>That simple change will format the feedback like this:</p> <p></p> <p>That in itself isn't very useful. It's exactly what was shown on the sidebar before. Thankfully, it's possible to do much more.</p> <p></p> <p>Screenshot from the Autolab Project docs</p> <p>Read more about formatted feedback here.</p>"},{"location":"Create%20an%20autograder/#student-metadata","title":"Student Metadata","text":"<p>Along with the student's submission, you'll receive a JSON file named <code>settings.json</code> that contains metadata about the student. The format of this file looks like:</p> <pre><code>{\"ubit\":\"username\",\"ip\":\"1.1.1.1\",\"lecture\":\"\",\"section\":\"\",\"timestamp\":\"2023-07-11T13:32:00.000-04:00\",\"version\":8,\"github_repo\":\"username-repo\",\"github_branch\":\"main\",\"github_commit\":\"1234abc\"}}\n</code></pre> <p>This can be used to verify a student is submitting at the right time or from the right location.</p> <p>Note that the \"lecture\" value may be <code>null</code>, while the \"section\" value will always be a string. The Autolab developers defined the schema as follows:</p> <pre><code>t.string \"lecture\"\nt.string \"section\", default: \"\"\n</code></pre> <p>GitHub submission metadata was added in September 2024. When a submission is NOT made from GitHub, \"github_repo\", \"github_branch\", and \"github_commit\" will be <code>null</code>.</p> <p>A sample grader that simply prints this information is available in <code>sample_files/autograder2</code>.</p>"},{"location":"Create%20an%20autograder/#embedded-forms","title":"Embedded Forms","text":"<p>Instead of having students submit a file, you can have them fill out an HTML form and pass the data to your grader.</p> <p>A sample embedded form grader is available in <code>sample_files/autograder3</code>.</p>"},{"location":"Create%20an%20autograder/#create-the-html-form","title":"Create the HTML form","text":"<p>First, let's create an HTML form. I've created a sample form in <code>sample_files/autograder3/form.html</code>. Notice that it only contains the inner HTML of the form tag. Autolab will automatically insert this file's contents into a form element.</p> <p>The <code>name</code> attribute on each input is the key that will be used in the JSON file.</p> <p>The following names are reserved (for settings.json metadata and Rails internals) and cannot be used:</p> <ul> <li>ubit</li> <li>ip</li> <li>lecture</li> <li>section</li> <li>timestamp</li> <li>version</li> <li>controller</li> <li>action</li> <li>submission</li> <li>utf8</li> <li>authenticity_token</li> <li>submission_file (only allowed for an embedded form with a file submission)</li> <li>integrity_checkbox</li> <li>course_name</li> <li>name</li> <li>github_repo</li> <li>github_branch</li> <li>github_commit</li> </ul> <p>Note that the label MUST come after the input, or it will be rendered incorrectly. This is worthy of mention because it's different from the Mozilla docs.</p> <p>This is correct:</p> <pre><code> &lt;label&gt;\n    &lt;input name=\"q2\" type=\"radio\" value=\"Yes\"/&gt;\n    &lt;span&gt;Yes&lt;/span&gt;\n&lt;/label&gt;\n</code></pre> <p>This is invalid:</p> <pre><code> &lt;label&gt;\n    &lt;span&gt;Yes&lt;/span&gt;\n    &lt;input name=\"q2\" type=\"radio\" value=\"Yes\"/&gt;\n&lt;/label&gt;\n</code></pre>"},{"location":"Create%20an%20autograder/#upload-the-html-form","title":"Upload the HTML form","text":"<p>After creating the form, navigate to the <code>Advanced</code> tab of the assessment options.</p> <ol> <li>Enable the embedded form</li> <li>Upload the embedded form</li> <li>Click <code>Save</code></li> </ol> <p></p> <p>From the <code>Handin</code> tab, change the <code>Handin filename</code> to <code>handin.json</code>. This isn't strictly necessary, but it's good practice.</p>"},{"location":"Create%20an%20autograder/#considerations-for-the-autograder","title":"Considerations for the autograder","text":"<p>Your grader will receive a file with the following contents:</p> <pre><code>{\"q1\":\"apple\",\"q2\":\"Yes\",\"q3\":\"99\"}\n</code></pre> <p>(Updated July 31, 2023: Extra fields professors don't care about have been removed. The output is now a very clean JSON map.)</p> <p>Your grader will need to parse that JSON and grade the submission accordingly. Notice it may contain non-ASCII characters. See the note below about encodings.</p> <p>After writing and uploading a grader, visit the assessment page, and you'll see a preview of the form. Fill it out and submit it. It'll be treated like a regular autograded assignment from here, as if the student uploaded the JSON file.</p> <p></p> <p>The feedback for our sample grader looks like this. Note that these are different inputs than the screenshot above.</p> <p></p> <ul> <li>Be aware that students can submit any data they want. The form isn't validated by Autolab. For example, a multiple   choice question can be answered with any string. This may be used intentionally to teach about POST requests in a   course   like CSE 312: Web Applications, so I won't demonstrate it here.</li> <li>Be careful with different encodings. It's safest to remove non-ASCII characters from the student's input. Definitely   test submitting with emojis to see how it behaves. \ud83d\ude0e</li> <li>Also test submitting a blank form.</li> </ul> <p>There's more information about embedded forms here: https://docs.autolabproject.com/features/embedded-forms/</p>"},{"location":"Create%20an%20autograder/#embedded-form-with-file-submission-ub-feature","title":"Embedded form with file submission (UB Feature)","text":"<p>If you need students to submit a file with an embedded form, create an embedded form with a file input named \"submission_file\".</p> <pre><code>&lt;input type=\"file\" name=\"submission_file\" required&gt;\n</code></pre> <p>This file will become the student's handin, and their form data will become part of their settings.json metadata.</p> <p>If a file is not attached, it will be treated as a regular embedded form submission; the form data will be the handin file, which will probably cause your autograder to fail. The <code>required</code> HTML attribute can prevent this from occurring accidentally. This is only client-side validation, so it's not a substitute for proper validation in your autograder.</p> <p>To preserve backwards compatibility, the form data is merged into the root of the settings.json map. A sample settings.json when creating a form with a file looks like this:</p> <pre><code>{\"q1\":\"This is my answer to question 1\",\"ubit\":\"username\",\"ip\":\"1.1.1.1\",\"lecture\":null,\"section\":\"\",\"timestamp\":\"2023-07-31T15:43:27.000-04:00\",\"version\":331,\"github_repo\":null,\"github_branch\":null,\"github_commit\":null}}\n</code></pre> <p>You can see a full example of this in <code>sample_files/autograder4</code>.</p> <p>Note: There isn't a convenient way to access the form data after submitting. If you want students, TAs, or yourself to be able to review the form data later, it's recommended to print it in the autograder feedback.</p>"},{"location":"Create%20an%20autograding%20image/","title":"Create an Autograding Image","text":"<p>An autograding image is a template for the sandboxed environment that submissions will run in. It defines which operating system will be used and which software is available while running an autograder. It should not contain any project-specific information.</p> <p>If you're using a common tech stack, there's a good chance you won't need to create a custom image. You can see all the current images in our Autograding Images repository.</p> <p>Before creating your own image, it's a good idea to be somewhat familiar with The autograding process.</p>"},{"location":"Create%20an%20autograding%20image/#creating-your-own-image","title":"Creating your own image","text":"<p>I'll walk through the process of a minimal Python grading image. You can also review a full example Dockerfile here.</p> <p>Create a Dockerfile. This will contain commands to initially set up the sandboxed environment for grading. It should NOT contain any project-specific details; that is left to your autograder.</p> <p>The first line needs to specify a base image that you're starting from. The latest LTS version of Ubuntu is usually a good choice. Add the <code>LABEL org.opencontainers.image.authors</code> line with your name/email address. Formatting conventions are more detailed in the Autograding Images repository.</p> <pre><code>FROM ubuntu:24.04\nLABEL org.opencontainers.image.authors=\"Nicholas Myers\"\n</code></pre> <p>Then, install the necessary Autodriver by pasting this exact block into your Dockerfile:</p> <pre><code># Install autodriver\nWORKDIR /home\nRUN useradd autolab\nRUN useradd autograde\nRUN mkdir autolab autograde output\nRUN chown autolab:autolab autolab\nRUN chown autolab:autolab output\nRUN chown autograde:autograde autograde\nRUN apt update\nRUN apt install -y sudo git make gcc\nRUN git clone https://github.com/autolab/Tango.git\nWORKDIR Tango/autodriver\nRUN make clean &amp;&amp; make\nRUN cp autodriver /usr/bin/autodriver\nRUN chmod +s /usr/bin/autodriver\nRUN apt -y autoremove\nRUN rm -rf Tango/\nWORKDIR /home\n</code></pre> <p>Installing the Autodriver early allows Docker to cache the layer to speed up iterative builds as you make modifications.</p> <p>Next, run any Linux commands necessary to build the environment. Avoid installing more packages than necessary. In this case, I'll just install Python and Pip.</p> <pre><code># Install Python3\nRUN apt update\nRUN apt install -y python3 python3-pip\n</code></pre> <p>At the end of the file, you can optionally run some debugging lines to verify software is installed properly. You'll only see this in the build logs. Students won't see any output from your Dockerfile when submitting.</p> <pre><code># Verify\nRUN python3 --version\n</code></pre> <p>Important: Your final <code>WORKDIR</code> MUST be <code>/home</code>! If you changed it after installing the Autodriver, add <code>WORKDIR /home</code> to the end of the file.</p> <p>Keep in mind that when a submission runs, the four files (your Makefile, your autograder, the submission, and the submission metadata) are in /home/autograde/autolab, and Make will be called in that directory as the autograde user. This doesn't affect this demo Dockerfile, but it may matter for more advanced setups. This is further explained in The autograding process.</p>"},{"location":"Create%20an%20autograding%20image/#testing-your-image","title":"Testing your image","text":"<p>Build your image with the next command. Replace <code>Dockerfile_cse_123</code> with your file name and <code>cse_123</code> with your image name. It should follow the same naming convention.</p> <pre><code>docker build -f Dockerfile_cse_123 -t cse_123 --progress=plain .\n</code></pre> <p>Run that command any time you update your Dockerfile.</p> <p>The <code>--progress=plain</code> option will show the output of your debugging lines.</p> <p>You can forcefully rebuild the entire image with the <code>--no-cache</code> option if changes don't seem to be applied.</p> <p>Create a directory that mimics how Tango will arrange the files.</p> <p>For example: in the <code>/path/to/directory</code> directory, I'll place:  <code>Makefile</code>, <code>handin.py</code>, and <code>autograde.tar</code>. You could also add <code>settings.json</code> if your grader depends on it. Autolab will always provide these four files. (The file name of the student submission is configured in your handin settings. The Makefile and autograde.tar come from your autograder.)</p> <p>To mimic running a job the way Tango will, use the following command.  * Update <code>cse_123</code> to your actual image name. * Update <code>/path/to/directory/</code> to the location you placed the 3 or 4 files.</p> <pre><code>docker run --rm -v /path/to/directory/:/home/mount cse_123 sh -c 'cp -r mount/* autolab/; su autolab -c \"autodriver -u 100 -f 104857600 -t 20 -o 1024000 autolab &gt; output/feedback 2&gt;&amp;1\"; cp output/feedback mount/feedback'\n</code></pre> <p>This complicated command is explained in depth in The autograding process. (Though, it's slightly different here for your convenience.)</p> <p>After running this command, view the autograder output from the <code>/path/to/directory/feedback</code> file.</p> <p>You can re-run the grader after updating the submission. The feedback file will be overwritten on each run.</p>"},{"location":"Create%20an%20autograding%20image/#add-your-image-to-autolab","title":"Add your image to Autolab","text":"<p>After verifying your image works with your autograder as expected, open a pull request with the image in the Autograding Images repository. Be sure to follow the formatting instructions in the README file.</p>"},{"location":"Getting%20started/","title":"Getting Started","text":""},{"location":"Getting%20started/#your-first-login","title":"Your First Login","text":"<p>The first time you visit Autolab at https://autolab.cse.buffalo.edu, you'll be asked to sign in.</p> <p></p> <p>Students and faculty should choose <code>Sign in with Shibboleth</code>. You'll be redirected to the UB Single Sign On page (shibboleth.buffalo.edu), where you can enter your standard UBIT credentials and perform a 2FA verification.</p> <p>After signing in, you'll be redirected to Autolab. Since you're a new user, you probably won't be enrolled in any courses. (Instructors can enroll students in courses before they've logged into Autolab for the first time, so you may see some courses even if this is your first time logging in.)</p> <p>If you're an instructor, you can Create a Course to get started. If you're a student, you'll need to wait for your instructor to create a course and enroll you in it.</p>"},{"location":"Getting%20started/#create-a-course","title":"Create a Course","text":"<p>This feature is only available to instructors who have current and/or future lectures or seminars officially scheduled by UB. If you need an Autolab course for a different purpose, please contact CSE IT.</p>"},{"location":"Getting%20started/#visit-the-portal","title":"Visit the portal","text":"<p>The footer of every page contains a link to our custom Autolab Portal.</p> <p></p> <p>Visit the portal, and click <code>Sign in with Shibboleth</code>. It will probably be automatic if you've recently signed into Autolab.</p> <p></p>"},{"location":"Getting%20started/#portal-home","title":"Portal home","text":"<p>After signing in, you'll see a list of actions you can perform. Select <code>Create a course</code>.</p> <p></p>"},{"location":"Getting%20started/#step-1","title":"Step 1","text":"<p>You'll see a list of current and upcoming courses you're scheduled to teach. Find the course you want to create, and click <code>Create this Course</code> at the bottom of the card.</p> <p></p>"},{"location":"Getting%20started/#about-course-urls","title":"About Course URLs","text":"<p>For consistency, the course URL is predefined based on the course name and semester. It's not customizable by the instructor. If a course is cross-listed, the lowest level course is used. The course URL is used for unique identification in different ways on the backend, so we try to refrain from deviating from this standard scheme.</p>"},{"location":"Getting%20started/#step-2","title":"Step 2","text":"<p>You'll be able to customize the course name. We've chosen a good default, but you can change it if you want. We have some guidelines and examples at the top of the page. Click <code>Continue</code> when it looks good.</p> <p></p>"},{"location":"Getting%20started/#step-3","title":"Step 3","text":"<p>Finally, you'll be able to confirm the course creation. If everything looks good, click <code>Confirm</code> at the bottom of the card. Most other options can be changed later if necessary.</p> <p></p>"},{"location":"Getting%20started/#success","title":"Success","text":"<p>If everything went well, you'll see a success message. You can click <code>Go to Course Page</code> to continue in Autolab. You can continue configuring your course with info in the Course Management guide.</p> <p></p>"},{"location":"Getting%20started/#if-theres-an-error","title":"If there's an error","text":"<p>If something went wrong, you'll see an error message. If the message doesn't help you resolve it, please contact CSE IT for assistance.</p> <p>Here's an example of an error message you might see if you try to create a course that already exists. In this case, it's likely that another instructor already created the course but hasn't added you to it yet. If you and another instructor would like separate Autolab courses for the same course in the same semester, please contact CSE IT. We've found that most instructors prefer to share a single course.</p> <p></p>"},{"location":"Getting%20started/#custom-course-url-conventions","title":"Custom course URL conventions","text":"<p>The URL conventions for automatically generated courses are described above. When courses are created manually, by contacting CSE IT, we still follow certain conventions for consistency.</p>"},{"location":"Getting%20started/#multiple-autolab-courses-for-a-single-ub-course","title":"Multiple Autolab courses for a single UB course","text":"<ul> <li>Faculty may request additional Autolab courses for different sections of a UB course.</li> <li>The first course should be created with the Self-service Portal like normal.<ul> <li>E.g., <code>cse442-f23</code></li> </ul> </li> <li>Subsequent courses can be created by contacting CSE IT.<ul> <li>The first 10 characters of the course URL are the same as the first course. This is parsed by automated tools,   such as the \"magic importer\" in the course sections tool.</li> <li>Reasonable suffixes are appended to the end of the standard course URL, such as section letters.<ul> <li>E.g., <code>cse442-f23b</code></li> <li>E.g., <code>cse442-f23c</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"Getting%20started/#individual-test-courses","title":"Individual test courses","text":"<ul> <li>CSE faculty and staff may request an Autolab course for testing purposes by contacting CSE IT.</li> <li>The course URL will be your username prefixed with <code>individual-</code>.<ul> <li>E.g., <code>individual-username</code></li> </ul> </li> <li>Only one Autolab course will be created per person.</li> </ul>"},{"location":"Getting%20started/#club-courses","title":"Club courses","text":"<ul> <li>Faculty advisors of   an officially recognized UB CSE/SEAS club   may request an Autolab course for the club by contacting CSE IT.</li> <li>The course URL will be the club name or abbreviation prefixed with <code>club-</code>.<ul> <li>E.g., <code>club-makeopensource</code></li> </ul> </li> <li>Only one Autolab course is allowed per club.</li> <li>Club courses may be used for testing Autolab or for actual club activities.</li> </ul>"},{"location":"Getting%20started/#project-development-test-courses","title":"Project development test courses","text":"<ul> <li>Faculty, staff, and personally approved students may request an Autolab course for development purposes by contacting   CSE IT.</li> <li>This is intended for developing external Autolab integrations, such as IDE extensions and custom apps that use the   Autolab API.</li> <li>The course URL will be your project name prefixed with <code>test-</code> followed by the semester.<ul> <li>E.g., <code>test-myproject-f23</code></li> </ul> </li> <li>Only one Autolab course will be created per project.</li> <li>If a project spans multiple semesters, the course URL will not change. The semester indicates when the project was   started.</li> <li>Since this is for testing purposes, you should not store any real student data in the course.</li> </ul>"},{"location":"Grader%20Assignment%20Tool/","title":"Grader Assignment Tool (GAT)","text":"<p>The Grader Assignment Tool is a custom UB application that allows instructors to assign a grader to each student's latest submission. It accounts for how many hours each grader works and which students each grader has a conflict of interest with.</p>"},{"location":"Grader%20Assignment%20Tool/#accessing-the-gat","title":"Accessing the GAT","text":"<p>The GAT is accessible via the Autolab Self-service Portal.</p> <p></p>"},{"location":"Grader%20Assignment%20Tool/#import-your-course-from-autolab","title":"Import your course from Autolab","text":"<p>The GAT is a completely separate application from Autolab, so you'll need to import your course to start using it.</p> <p>From the GAT index page, click the \"Import from Autolab\" button.</p> <p></p> <p>This will list all the courses you're enrolled in on Autolab. Find the course you want to import, and click the \"Import from Autolab\" button.</p> <p>You can only import courses where you're an instructor or course assistant. The entire GAT system treats instructors and course assistants equally. Both are considered \"graders\" and have the same access level to the whole tool.</p> <p></p> <p>Then, you can open the course with the convenient button that will appear after importing. Your new course will also be available at the top of the page.</p> <p></p>"},{"location":"Grader%20Assignment%20Tool/#manage-people","title":"Manage people","text":"<p>From the GAT course page, click the \"Manage People\" button.</p> <p></p> <p>The \"Manage People\" page will show the graders and students in your course.</p> <p></p> <p>You can assign how many hours each teaching assistant will work here. This is effectively just a weight for how many students they'll be assigned, so it doesn't strictly need to be in hours. You can assign yourself 0 hours to not be assigned any students to grade.</p> <p>The whole GAT application is designed to feel modern, so you can type in a number and it will automatically save.</p> <p>If you change your course roster on Autolab, you can click the \"Sync Roster from Autolab\" button to update the GAT roster. This will update both names and roles. Name updates work retroactively after creating a grading assignment. If new graders are added or removed, it will NOT update previously generated grading assignments.</p>"},{"location":"Grader%20Assignment%20Tool/#manage-conflicts-of-interest","title":"Manage conflicts of interest","text":"<p>There are two ways to access conflicts of interest:</p> <ol> <li>From the manage users page, click the \"COI\" link in someone's row to view a particular person's conflicts of    interest.</li> <li>From the GAT course page, click the \"My Conflicts of Interest\" button to view your own conflicts of interest.</li> </ol> <p>The conflicts of interest page for a particular user lists everyone in the course that the person could have a conflict of interest with. If the person is a grader, it will list the students, and vise versa.</p> <p></p> <p>You should have each of your graders fill out their own conflicts of interest before creating any grading assignments. They can do this by checking the box on each student they have a conflict of interest with. Changes are saved immediately. Graders will not be assigned to grade any students they have a conflict of interest with.</p>"},{"location":"Grader%20Assignment%20Tool/#create-a-grading-assignment","title":"Create a grading assignment","text":"<p>Once the roster is imported and conflicts of interest are filled out, you can create a grading assignment.</p> <p>A grading assignment is a mapping of graders to students for a particular assessment. Once a grading assignment is created, it cannot be changed. If you realize you generated a grading assignment with incorrect information, you can archive it and generate a new one. Nothing is ever permanently deleted to prevent data loss.</p> <p>Since grading assignments won't change after creation, you should not generate one until everybody has submitted their work to Autolab\u2014usually after the due date. If a student makes a submission after the grading assignment is created, that submission will not be considered. (A previous submission from the same student will still be included if it was made before the grading assignment.)</p> <p>From the GAT course page, click the \"Create New Grading Assignment\" button.</p> <p></p> <p>You'll see a list of all the Autolab assessments in your course.</p> <p></p> <p>You can view the assessment on Autolab by clicking the \"Autolab Link\" in the assessment's row. Click the \"Create\" link to create a grading assignment for that assessment. You'll need to confirm creation on the next page.</p> <p></p> <p>If this succeeds, then you can click the grading assignment to view it. If there was an error, it will tell you what went wrong.</p> <p></p>"},{"location":"Grader%20Assignment%20Tool/#view-a-grading-assignment","title":"View a grading assignment","text":"<p>On the GAT course page, you'll see a list of grading assignments. The newest ones are towards the top. Archived grading assignments are hidden by default, but you can show them with the \"Show archived grading assignments\" checkbox. Click one of the grading assignments to view it.</p> <p></p> <p>The grading assignment page will show you general information about the grading assignment, including:</p> <ul> <li>Who created it</li> <li>When it was created</li> <li>The unique ID</li> <li>A link to the Autolab assessment page</li> </ul> <p>The rest of the page lists each grader and the student submissions they're assigned to grade. The person viewing the page will always be shown first if they have students to grade.</p> <p></p> <p>You can see the 3 graders were assigned different numbers of students according to their hours. The first grader, Ca1 One, has conflicts of interest with students 2-5, so they weren't assigned to them. Student5 didn't submit this assessment, so they weren't assigned to a grader.</p> <p>Each row includes a link to the student's submission on Autolab, which mentions which version the submission is. Each grader can go through their list of students, click the link to their submission, grade the submission on Autolab, come back to the GAT page, and then check the \"Complete\" checkbox. Only the assigned grader can mark their own students as complete. GAT does not make any modifications on Autolab, and it doesn't handle scores.</p> <p></p> <p>You, as the instructor, can view your graders' progress on this page. As graders complete their assignments, their count will increase and the complete rows will be shaded. Once a grader has finished all their assignments, the entire table will turn gray, and a green \"Complete\" checkmark will appear.</p>"},{"location":"Grader%20Assignment%20Tool/#archive-a-grading-assignment","title":"Archive a grading assignment","text":"<p>Once a grading assignment is complete, you can archive it.</p> <p>To archive a grading assignment, check the \"Archive\" checkbox on the grading assignment page. It will turn gray and add an \"Archived\" label. This will hide it from the course page by default to avoid clutter.</p> <p></p> <p>Archived grading assignments can still be modified (by marking assignments as complete), and they can always be unarchived.</p>"},{"location":"The%20autograding%20process/","title":"The Autograding Process","text":"<p>Autolab follows a complex procedure for running an autograding job. Jobs are executed in sandboxed containers to ensure they can't interfere with shared resources and distributed across multiple servers to handle a large quantity of submissions.</p> <p>Tango is Autolab's autograding backend. It handles job queueing and distribution. It's a separate service from Autolab, but they're tightly integrated. Tango's API is documented by CMU.</p>"},{"location":"The%20autograding%20process/#sending-the-job-to-tango","title":"Sending the job to Tango","text":"<p>An autograding job is started when a user makes a submission to an assignment with an autograder configured or an instructor regrades an existing submission.</p> <p>To begin a job, Autolab first makes a GET request to Tango's <code>/open/&lt;key&gt;/&lt;course-assessment&gt;/</code> endpoint. Tango creates a directory for the assignment if it doesn't already exist. If the directory already exists, Tango returns the MD5 hash of every file in the directory so Autolab can avoid uploading duplicates (e.g., your autograder files).</p> <p>Next, Autolab repeatedly makes POST requests to Tango's <code>/upload/&lt;key&gt;/&lt;course-assessment&gt;/</code> endpoint for each (new; skipping MD5 collision duplicates) file the job requires.</p> <p>This includes:</p> <ul> <li>The autograder: <code>autograde.tar</code></li> <li>The Makefile: <code>autograde-Makefile</code></li> <li>The submission: E.g., <code>student@buffalo.edu_5_handin.py</code></li> <li>The submission metadata: E.g., <code>student@buffalo.edu_5_handin.py.settings.json</code></li> </ul> <p>Files are saved to <code>/opt/TangoService/Tango/courselabs/&lt;key&gt;-&lt;course&gt;-&lt;assignment&gt;/&lt;filename&gt;</code></p> <p>After uploading all required files, Autolab generates a random callback URL, which Tango will request when the job is finished.</p> <p>To queue the job, Autolab makes a POST request to Tango's <code>/addJob/&lt;key&gt;/&lt;course-assessment&gt;/</code> endpoint. This request contains the following information:</p> <ul> <li>The image name configured in the autograder settings</li> <li>The timeout configured in the autograder settings</li> <li>Each file required for grading</li> <li>This contains the original filename on Tango's filesystem (e.g., <code>student@buffalo.edu_5_handin.py</code>) and the destination filename (e.g., <code>handin.py</code>), which is what the file will be named within the container when grading.</li> <li>The name of the feedback output file (generated by Autolab, ends with <code>_autograde.txt</code>)</li> <li>The callback URL mentioned above (generated by Autolab)</li> <li>The job name (generated by Autolab)</li> </ul> <p>Tango validates the job (e.g., all required files exist, the image exists, etc.) and returns an error if there is one. Error messages are propagated back to Autolab. Autolab will reveal more detailed errors to instructors, while students will see vague errors.</p>"},{"location":"The%20autograding%20process/#tango-queues-the-job","title":"Tango queues the job","text":"<p>If the job is valid, Tango adds it to the job queue. Tango has a configured number of container instances that can run simultaneously for each image. The job will remain in the queue until there is an instance of the required image available. Usually, jobs are dequeued immediately, but during high-traffic times, such as assignment deadlines, the queue may get longer.</p>"},{"location":"The%20autograding%20process/#running-the-job","title":"Running the job","text":"<p>When a job is ready to run, Tango:</p> <ul> <li>Picks a worker node to run the job on</li> <li>Creates a directory on the worker to copy the job files to via SSH</li> <li>E.g., <code>/docker-volumes/dev-1005-autograding_image</code></li> <li>Copies each job file to the worker node via SCP</li> <li>E.g., <code>/docker-volumes/dev-1005-autograding_image/handin.py</code></li> <li>Executes the command to begin autograding on the worker node via SSH (elaborated below)</li> </ul> <p>The command that begins autograding is similar to the following. This example is from a development environment, but the core of the command is identical in production.</p> <pre><code>(docker run --name dev-1005-autograding_image -v /docker-volumes/dev-1005-autograding_image/:/home/mount autograding_image sh -c 'cp -r mount/* autolab/; su autolab -c \"autodriver -u 100 -f 104857600 -t 20 -o 1024000 autolab &gt; output/feedback 2&gt;&amp;1\"; cp output/feedback mount/feedback')\n</code></pre> <p>Let's break that complicated command down:</p> <ul> <li>It creates and starts a new Docker container from the image you specify (e.g., <code>autograding_image</code>)</li> <li>The directory with the files required for the job is mounted to <code>/home/mount</code> within the container</li> <li>Within the container, the following command is executed: <code>cp -r mount/* autolab/; su autolab -c \"autodriver -u 100 -f 104857600 -t 20 -o 1024000 autolab &gt; output/feedback 2&gt;&amp;1\"; cp output/feedback mount/feedback</code></li> <li>All the input files are copied to the <code>/home/autolab</code> directory<ul> <li>The final <code>WORKDIR</code> in your Dockerfile should always be <code>/home</code>. This document assumes it is.</li> </ul> </li> <li>It switches to the <code>autolab</code> user, which must be created in your Dockerfile. (Up until this point, it has been running as whichever user you specified in your Dockerfile, or usually <code>root</code> by default.)</li> <li>The <code>autolab</code> user executes <code>autodriver -u 100 -f 104857600 -t 20 -o 1024000 autolab &gt; output/feedback 2&gt;&amp;1</code><ul> <li>(These limits are different in production)</li> <li><code>-u 100</code> limits the number of processes that can be started</li> <li><code>-f 104857600</code> sets the maximum file size that can be created</li> <li><code>-t 20</code> sets the job timeout</li> <li><code>-o 1024000</code> limits the size of the output</li> <li><code>autolab</code> specifies the directory that will be copied into the grading user's home directory</li> <li><code>&gt; output/feedback 2&gt;&amp;1</code> saves stdout and stderr to the feedback file</li> </ul> </li> <li>(autodriver is explained in depth below)</li> <li>After grading, we're back to the original user, and the feedback file is copied to <code>/home/mount/feedback</code>, which is shared with the host</li> </ul>"},{"location":"The%20autograding%20process/#autodriver","title":"Autodriver","text":"<p>The autodriver configures the environment in the container before running your autograder.</p> <p>It does a lot, but the most important parts are:</p> <ul> <li>It is running as the <code>autolab</code> user (but the submission code is NOT; keep reading)</li> <li>It moves the files from <code>/home/autolab</code> to <code>/home/autograde/autolab</code></li> <li>It sets <code>/home/autograde/autolab</code> as the CWD</li> <li>It changes ownership of the <code>/home/autograde</code> directory recursively to the <code>autograde</code> user</li> <li>It forks a child process to run the instructor's autograder with limited privileges</li> <li>The child process runs as the <code>autograde</code> user, but the environment is NOT updated<ul> <li>This means that running <code>whoami</code> prints <code>autograde</code>, but the environment variables are: <code>USER: autolab</code> and <code>HOME: /home/autolab</code></li> </ul> </li> <li>The child process' stdout and stderr are redirected to <code>/home/autograde/output.log</code></li> <li>The child process calls <code>Make</code> to begin running your autograder</li> <li>It exits after Make either successfully exits or times out</li> </ul> <p>The main takeaway is that the four files (your Makefile, your autograder, the submission, and the submission metadata) are in <code>/home/autograde/autolab</code>, and Make will be called in that directory as the <code>autograde</code> user.</p>"},{"location":"The%20autograding%20process/#after-the-job-is-finished","title":"After the job is finished","text":"<p>After a job finishes, Tango:</p> <ul> <li>Copies the feedback file (e.g., <code>/docker-volumes/dev-1005-autograding_image/feedback</code>) from the worker node to its filesystem (e.g., <code>/opt/TangoService/Tango/courselabs/&lt;key&gt;-&lt;course&gt;-&lt;assignment&gt;/output/student@buffalo.edu_5_assignment_autograde.txt</code>) via SCP</li> <li>Destroys the Docker container and the volume (e.g., <code>/docker-volumes/dev-1005-autograding_image/</code>) on the worker node</li> <li>Removes the job from the queue and disassociates the worker node from the job container</li> <li>Creates a temporary file combining a header (with the job history and status) and the feedback file</li> <li>Makes a POST request to Autolab's callback URL with the result file to notify Autolab that the job finished</li> </ul> <p>Autolab assigns the feedback from Tango to the submission (in the database) and saves it to the filesystem.</p> <p>Autograding is complete!</p> <p>(And all of this happens in under 3 seconds for minimal autograders!) </p>"},{"location":"UB%20course%20sections/","title":"UB Course Sections","text":"<p>The UB Course Sections Autolab feature allows you to restrict submission times in a more granular way. Submissions can be only allowed during a particular lecture or section meeting time. Offsets can be applied to further specify the time each lecture/section is allowed to submit. Attachments can be restricted to only be accessible when a student is allowed to submit.</p> <p>\"Lecture\" refers to the main lecture for the course, which typically meets multiple times per week.</p> <p>\"Section\" may refer to labs, recitation, or anything else you'd like to use this for, which typically meets once per week.</p> <p>This entire feature requires being an \"Instructor\" in the Autolab course.</p>"},{"location":"UB%20course%20sections/#configure-your-course-sections","title":"Configure your course sections","text":"<p>To begin using the UB Course Sections feature, you must first configure your course sections. This is done in the Autolab Self-service Portal.</p> <p>From the Portal home page, choose \"Manage Course Sections.\"</p> <p></p> <p>Choose the course you want to configure sections for.</p> <p></p>"},{"location":"UB%20course%20sections/#magic-import","title":"Magic import","text":"<p>For most courses, you'll be able to click \"Magic Import,\" which will automatically import the course sections from UB's course database.</p> <p></p> <p>In this case, this sample course doesn't really exist, so we'll need to create the sections manually.</p>"},{"location":"UB%20course%20sections/#create-a-section-manually","title":"Create a section manually","text":"<p>Under both the \"lectures\" and \"sections\" tables, type a lecture/section name into the \"New ____ Name\" field and click \" Add Lecture\" (or press enter).</p> <p>This will add an empty row to the table. You'll need to set the start time, end time, and days of the week for the section. These will not save automatically, but the UI will make it clear that you have unsaved changes.</p> <p></p> <p>Tip: Click the clock icon to open a visual time picker. The time must be in 24-hour 00:00:00 format. It will always be interpreted in the timezone we're in (not UTC), so it will remain consistent with daylight savings time.</p> <p>When you're done making changes, click the \"Save\" button at the top.</p> <p>Lecture/section names must match the ones in the Autolab roster:</p> <p></p>"},{"location":"UB%20course%20sections/#configure-an-assessment","title":"Configure an assessment","text":"<p>Once you've configured your course sections, you can configure an assessment to use them.</p> <p>Create an assessment on Autolab. Then, navigate to the \"Handin\" tab of the assessment options.</p> <p></p> <p>Configure the assessment to your liking. Here's a numbered description of the options:</p> <ol> <li>Check this box to enable the UB Course Sections feature.</li> <li>Check this box to use the \"lecture\" field. If this isn't checked, the \"section\" will be used instead.</li> <li>Check this box to only allow viewing assessment attachments during the allowed submission time.</li> <li>Choose a section start offset time.</li> <li>Choose a section end offset time.</li> </ol> <p>Save the assessment.</p>"},{"location":"UB%20course%20sections/#how-it-looks-to-students","title":"How it looks to students","text":"<p>Here's a student who's in lecture A, which begins at 1:00 PM and ends at 1:50 PM on Monday, Wednesday, and Friday. The current time is Monday at 4 PM, so they're unable to submit. Here's what they'll see with the options we configured above.</p> <p></p> <p>For a \"lab exam,\" we'd want to use lab sections instead of lecture sections. This student is in section A1, which begins at 12:00 PM and ends at 2:00PM on Monday If we set the assessment to use sections instead of lectures and reset both offsets to 0, here's what the student would see:</p> <p></p> <p>The <code>Manage Course &gt; Act as user</code> feature may be helpful for testing your configuration.</p> <p>When the student is allowed to submit, they'll see the regular handin page.</p>"}]}